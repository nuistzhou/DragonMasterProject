---
title: "Dragon Master Project"
output:
  html_document: default
  html_notebook: default
---
This is the final project of the Geoscripting course, period 3, 2016-2017, Wageningen University.  
Authors: Rodrigo Almeida and Ping Zhou  
January 2016  
  
__Main goal:__ Find the best country/region on earth according to a living quality index based on:   
1. Hazard frequency and distribution;  
2. Air pollution;  
3. GDP;  
4. Average year NDVI.  
  
__Warning:__ Running the entire script takes about 2 hours of computation time in the 2 core virtual machine we used. We highly recommend using the terminal.

## Setup  
```{r, include = FALSE}
knitr::read_chunk('main.R')
knitr::read_chunk('R/ndvi_annual_mean.R')
knitr::read_chunk('R/summary_data.R')
knitr::read_chunk('R/normalization.R')
knitr::read_chunk('R/hazards_sum.R')
knitr::read_chunk('R/calc_index.R')
knitr::read_chunk('R/vis.R')
```
Our project core is located at main.R file. First, we start by setting some environment settings, including he installation of necessary libraries and the import of such libraries. We also source the sub-scripts in the setup section.  
```{r setup, eval=FALSE}
```
## Downloads  
For downloading the necessary datasets, we used two bash scripts called from the main script and a simple download.file function call.   
```{r downloads, eval = FALSE}
```
For the porpuses of the SEDAC data downloaded we used the following wget command.
```{bash, eval = FALSE}
# Example for Drought Hazard dataset
wget -L --user=rodrigoalmeida94 --password=*RmA20071994**** --load-cookies ~/.cookies --save-cookies ~/.cookies --no-directories http://sedac.ciesin.columbia.edu/downloads/data/ndh/ndh-drought-hazard-frequency-distribution/gddrg.zip
# Unzips and places files in the right place
unzip gddrg.zip -d ../data/
rm gddrg.zip
mv ../data/gddrg/gddrg.asc ../data/haz_drought.asc
mv ../data/gddrg/gddrg.prj ../data/haz_drought.prj
rm -r ../data/gddrg
```
For the MODIS NDVI product, we made a for loop that goes through every folder in the DAAC for 2016 and downloads the respective file (0.05 degrees resolution).
```{bash, eval = FALSE}
for i in 01 02 03 04 05 06 07 08 09 10 11 12;
do wget -L --user=rodrigoalmeida94 --password=**** --load-cookies ~/.cookies --save-cookies ~/.cookies -r --no-parent -A '*.hdf' --no-directories -P ../data/http://e4ftl01.cr.usgs.gov/MOLT/MOD13C2.006/2016.$i.01/
done
```
The GECON datasets were available in SEDAC as well, but unfortunately the data had some issues, so we decided to make the raster layer ourselfs from the XLS file provided in the project website.  

## Read files
The process of reading the files into memory is quite straighforward:  
1. Loading all the hazard files with the correct name using a for loop;  
2. Loading the NDVI monthly data, and passing it to the annual_mean function, that returns the annual mean;  
3. Loading the PM2.5 dataset into memory; 
4. Creating the GECON data with the XLS file into two rasters GECON PPP and GECON MER.  
```{r read-files, eval=FALSE, tidy=TRUE}

```
The annual mean function for the NDVI takes the month dataset, cleans the data according to the reliability layer that is provided with the MODIS NDVI product and calculates the mean.  
```{r ndvi_annual_mean, eval=FALSE, tidy=TRUE}

```
The creation of the GECON dataset from the XLS file was a bit trickier than expected, since the gridded takes the coordinates as the center of the cell coordinates, and in this case it was the coordinates of the corners of the cell. Because of that it was necessary to add 1 degree to the existing coordinates to correct for that.  

## Files information
In order to obtain an overview of the downloaded files, we created a function to report all attributes of the data sets in a data frame format.  
```{r summary_data, eval=FALSE}

```




